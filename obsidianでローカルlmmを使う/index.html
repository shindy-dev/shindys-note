<!doctype html><html lang=ja-JP><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Shindy's Note</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="ObsidianでローカルLLMを使う
Obsidianの拡張機能とHugging Face1、Ollama2を使用してオフライン環境下での生産性の爆上げを図る。
※自己責任で実施すること。
flowchart LR
A(Hugging FaceでLLMをフォーク) --> B(OllamaでLLMをpull&ローカル実行)
B --> C(Obsidianの拡張機能でLLMを実行)
ちなみに表題に反して、おすすめはGemini APIです。
実装環境
M3 Macbook
Hugging FaceでLLMをフォーク
自分の使いたいLLMがいつ配信停止となっても困らないように、自分のリポジトリにフォークする。
※Hugging FaceにはOllamaで使えない形式のモデルも存在するため選定の際は注意すること。
※いつ配信停止となっても別のモデルを探せるのであればObsidianでローカルLMMを使うから実施すること。
Hugging Face アカウント の作成
Hugging Faceのアカウントはメールアドレスがあれば作れる。有料プランを使用する場合は乗っ取り防止で2FAの設定をしておいた方がいい。
Hugging Face – The AI community building the future.
Hugging Face CLI のインストール
Hugging FaceのGUIだと日本語対応しておらず、UIも変更される可能性もあるため、再現性を確保するためCLIをインストールする。
brew install huggingface-cli
Access Tokens の作成
Settingsの[Access Tokens](Hugging Face – The AI community building the future.)から作成する。Repositories内の項目をチェックする。トークンは発行時にしか表示されないため、メモしておくこと。

Hugging Face CLI でログイン
先ほど発行したトークンでログインする。
hf auth login --token <トークン>
Git の大容量ファイル対応
gitが大規模言語モデル等の大容量ファイルを操作できるようにgit-lfsをインストールする。"><meta name=generator content="Hugo 0.148.2"><meta name=robots content="index, follow"><meta name=author content="shindy"><link rel=stylesheet href=/shindys-note/ananke/css/main.min.8d048772ae72ab11245a0e296d1f2a36d3e3dd376c6c867394d6cc659c68fc37.css><link rel=canonical href=https://shindy-dev.github.io/shindys-note/obsidian%E3%81%A7%E3%83%AD%E3%83%BC%E3%82%AB%E3%83%ABlmm%E3%82%92%E4%BD%BF%E3%81%86/><meta property="og:url" content="https://shindy-dev.github.io/shindys-note/obsidian%E3%81%A7%E3%83%AD%E3%83%BC%E3%82%AB%E3%83%ABlmm%E3%82%92%E4%BD%BF%E3%81%86/"><meta property="og:site_name" content="Shindy's Note"><meta property="og:title" content="Shindy's Note"><meta property="og:description" content="ObsidianでローカルLLMを使う Obsidianの拡張機能とHugging Face1、Ollama2を使用してオフライン環境下での生産性の爆上げを図る。
※自己責任で実施すること。
flowchart LR A(Hugging FaceでLLMをフォーク) --> B(OllamaでLLMをpull&ローカル実行) B --> C(Obsidianの拡張機能でLLMを実行) ちなみに表題に反して、おすすめはGemini APIです。
実装環境 M3 Macbook
Hugging FaceでLLMをフォーク 自分の使いたいLLMがいつ配信停止となっても困らないように、自分のリポジトリにフォークする。
※Hugging FaceにはOllamaで使えない形式のモデルも存在するため選定の際は注意すること。 ※いつ配信停止となっても別のモデルを探せるのであればObsidianでローカルLMMを使うから実施すること。
Hugging Face アカウント の作成 Hugging Faceのアカウントはメールアドレスがあれば作れる。有料プランを使用する場合は乗っ取り防止で2FAの設定をしておいた方がいい。
Hugging Face – The AI community building the future.
Hugging Face CLI のインストール Hugging FaceのGUIだと日本語対応しておらず、UIも変更される可能性もあるため、再現性を確保するためCLIをインストールする。
brew install huggingface-cli Access Tokens の作成 Settingsの[Access Tokens](Hugging Face – The AI community building the future.)から作成する。Repositories内の項目をチェックする。トークンは発行時にしか表示されないため、メモしておくこと。 Hugging Face CLI でログイン 先ほど発行したトークンでログインする。
hf auth login --token <トークン> Git の大容量ファイル対応 gitが大規模言語モデル等の大容量ファイルを操作できるようにgit-lfsをインストールする。"><meta property="og:locale" content="ja_JP"><meta property="og:type" content="article"><meta property="article:published_time" content="2025-08-15T20:41:20+00:00"><meta property="article:modified_time" content="2025-08-15T20:41:20+00:00"><meta property="article:tag" content="HuggingFace"><meta property="article:tag" content="Ollama"><meta property="article:tag" content="Obsidian"><meta itemprop=name content="Shindy's Note"><meta itemprop=description content="ObsidianでローカルLLMを使う Obsidianの拡張機能とHugging Face1、Ollama2を使用してオフライン環境下での生産性の爆上げを図る。
※自己責任で実施すること。
flowchart LR A(Hugging FaceでLLMをフォーク) --> B(OllamaでLLMをpull&ローカル実行) B --> C(Obsidianの拡張機能でLLMを実行) ちなみに表題に反して、おすすめはGemini APIです。
実装環境 M3 Macbook
Hugging FaceでLLMをフォーク 自分の使いたいLLMがいつ配信停止となっても困らないように、自分のリポジトリにフォークする。
※Hugging FaceにはOllamaで使えない形式のモデルも存在するため選定の際は注意すること。 ※いつ配信停止となっても別のモデルを探せるのであればObsidianでローカルLMMを使うから実施すること。
Hugging Face アカウント の作成 Hugging Faceのアカウントはメールアドレスがあれば作れる。有料プランを使用する場合は乗っ取り防止で2FAの設定をしておいた方がいい。
Hugging Face – The AI community building the future.
Hugging Face CLI のインストール Hugging FaceのGUIだと日本語対応しておらず、UIも変更される可能性もあるため、再現性を確保するためCLIをインストールする。
brew install huggingface-cli Access Tokens の作成 Settingsの[Access Tokens](Hugging Face – The AI community building the future.)から作成する。Repositories内の項目をチェックする。トークンは発行時にしか表示されないため、メモしておくこと。 Hugging Face CLI でログイン 先ほど発行したトークンでログインする。
hf auth login --token <トークン> Git の大容量ファイル対応 gitが大規模言語モデル等の大容量ファイルを操作できるようにgit-lfsをインストールする。"><meta itemprop=datePublished content="2025-08-15T20:41:20+00:00"><meta itemprop=dateModified content="2025-08-15T20:41:20+00:00"><meta itemprop=wordCount content="175"><meta itemprop=keywords content="HuggingFace,Ollama,Obsidian"><meta name=twitter:card content="summary"><meta name=twitter:title content="Shindy's Note"><meta name=twitter:description content="ObsidianでローカルLLMを使う Obsidianの拡張機能とHugging Face1、Ollama2を使用してオフライン環境下での生産性の爆上げを図る。
※自己責任で実施すること。
flowchart LR A(Hugging FaceでLLMをフォーク) --> B(OllamaでLLMをpull&ローカル実行) B --> C(Obsidianの拡張機能でLLMを実行) ちなみに表題に反して、おすすめはGemini APIです。
実装環境 M3 Macbook
Hugging FaceでLLMをフォーク 自分の使いたいLLMがいつ配信停止となっても困らないように、自分のリポジトリにフォークする。
※Hugging FaceにはOllamaで使えない形式のモデルも存在するため選定の際は注意すること。 ※いつ配信停止となっても別のモデルを探せるのであればObsidianでローカルLMMを使うから実施すること。
Hugging Face アカウント の作成 Hugging Faceのアカウントはメールアドレスがあれば作れる。有料プランを使用する場合は乗っ取り防止で2FAの設定をしておいた方がいい。
Hugging Face – The AI community building the future.
Hugging Face CLI のインストール Hugging FaceのGUIだと日本語対応しておらず、UIも変更される可能性もあるため、再現性を確保するためCLIをインストールする。
brew install huggingface-cli Access Tokens の作成 Settingsの[Access Tokens](Hugging Face – The AI community building the future.)から作成する。Repositories内の項目をチェックする。トークンは発行時にしか表示されないため、メモしておくこと。 Hugging Face CLI でログイン 先ほど発行したトークンでログインする。
hf auth login --token <トークン> Git の大容量ファイル対応 gitが大規模言語モデル等の大容量ファイルを操作できるようにgit-lfsをインストールする。"></head><body class="ma0 avenir bg-near-white production"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l center items-center justify-between"><a href=/shindys-note/ class="f3 fw2 hover-white white-90 dib no-underline">Shindy's Note</a><div class="flex-l items-center"><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l mw8 center ph3 flex-wrap justify-between"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked ttu">Shindy's Note</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1"></h1><p class=tracked><strong>shindy</strong></p><time class="f6 mv4 dib tracked" datetime=2025-08-15T20:41:20Z>August 15, 2025</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h1 id=obsidianでローカルllmを使う>ObsidianでローカルLLMを使う</h1><p>Obsidianの拡張機能と<a href=https://huggingface.co/>Hugging Face</a><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>、<a href=https://ollama.com/>Ollama</a><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>を使用してオフライン環境下での生産性の爆上げを図る。<br>※自己責任で実施すること。</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>flowchart LR
A(Hugging FaceでLLMをフォーク) --&gt; B(OllamaでLLMをpull&amp;ローカル実行)
B --&gt; C(Obsidianの拡張機能でLLMを実行)
</code></pre><p>ちなみに表題に反して、おすすめはGemini APIです。</p><h2 id=実装環境>実装環境</h2><p>M3 Macbook</p><h2 id=hugging-faceでllmをフォーク>Hugging FaceでLLMをフォーク</h2><p>自分の使いたいLLMがいつ配信停止となっても困らないように、自分のリポジトリにフォークする。<br>※Hugging FaceにはOllamaで使えない形式のモデルも存在するため選定の際は注意すること。
※いつ配信停止となっても別のモデルを探せるのであれば<a href=#Ollama%E3%81%A7LLM%E3%82%92pull&%E3%83%AD%E3%83%BC%E3%82%AB%E3%83%AB%E5%AE%9F%E8%A1%8C>ObsidianでローカルLMMを使う</a>から実施すること。</p><h3 id=hugging-face-アカウント-の作成>Hugging Face アカウント の作成</h3><p>Hugging Faceのアカウントはメールアドレスがあれば作れる。有料プランを使用する場合は乗っ取り防止で2FAの設定をしておいた方がいい。<br><a href=https://huggingface.co/>Hugging Face – The AI community building the future.</a></p><h3 id=hugging-face-cli-のインストール>Hugging Face CLI のインストール</h3><p>Hugging FaceのGUIだと日本語対応しておらず、UIも変更される可能性もあるため、再現性を確保するためCLIをインストールする。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install huggingface-cli
</span></span></code></pre></div><h3 id=access-tokens-の作成>Access Tokens の作成</h3><p>Settingsの[Access Tokens](<a href=https://huggingface.co/settings/tokens>Hugging Face – The AI community building the future.</a>)から作成する。Repositories内の項目をチェックする。トークンは発行時にしか表示されないため、メモしておくこと。
<img src=../assets/Pasted%20image%2020250816215531.png alt=600></p><h3 id=hugging-face-cli-でログイン>Hugging Face CLI でログイン</h3><p>先ほど発行したトークンでログインする。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>hf auth login --token &lt;トークン&gt;
</span></span></code></pre></div><h3 id=git-の大容量ファイル対応>Git の大容量ファイル対応</h3><p>gitが大規模言語モデル等の大容量ファイルを操作できるように<code>git-lfs</code>をインストールする。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install git-lfs
</span></span><span style=display:flex><span>git lfs install
</span></span></code></pre></div><h3 id=llm-のフォーク>LLM のフォーク</h3><p>フォークといってもGitHubみたいにスマートにはできないため、以下のように行う。（フォーク元のライセンスはあらかじめ確認すること。）</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># リポジトリの作成</span>
</span></span><span style=display:flex><span>hf repo create &lt;フォーク先のリポジトリ&gt;
</span></span><span style=display:flex><span><span style=color:#75715e># ベースモデルのダウンロード（ダウンロード先は標準出力される）</span>
</span></span><span style=display:flex><span>hf download &lt;使いたいLLMリポジトリ&gt;
</span></span><span style=display:flex><span><span style=color:#75715e># ベースモデルをそのまま作成したリポジトリにアップロード</span>
</span></span><span style=display:flex><span>hf upload &lt;フォーク先のリポジトリ&gt; &lt;ベースモデルのダウンロード先&gt;
</span></span></code></pre></div><p>今回は以下2つの日本語学習済みモデルをフォークする。</p><table><thead><tr><th>モデル</th><th>用途</th><th>サイズ</th></tr></thead><tbody><tr><td><code>elyza/Llama-3-ELYZA-JP-8B-GGUF</code></td><td>大規模言語モデル（会話生成、質問応答など）</td><td>約4.92GB</td></tr><tr><td><code>lmstudio-community/granite-embedding-278m-multilingual-GGUF</code></td><td>文書埋め込みモデル（類似度計算、検索など）</td><td>約972MB</td></tr></tbody></table><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># Llama-3-ELYZA-JP-8B-GGUFのフォーク</span>
</span></span><span style=display:flex><span>hf repo create Llama-3-shindy-jp-8B-GGUF
</span></span><span style=display:flex><span>hf download elyza/Llama-3-ELYZA-JP-8B-GGUF
</span></span><span style=display:flex><span>hf upload shindy-dev/Llama-3-shindy-jp-8B-GGUF &lt;ベースモデルのダウンロード先&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ruri-largeのフォーク</span>
</span></span><span style=display:flex><span>hf repo create granite-embedding-278m-shindy-multilingual-GGUF
</span></span><span style=display:flex><span>hf download lmstudio-community/granite-embedding-278m-multilingual-GGUF
</span></span><span style=display:flex><span>hf upload shindy-dev/granite-embedding-278m-shindy-multilingual-GGUF &lt;ベースモデルのダウンロード先&gt;
</span></span></code></pre></div><p>自分の作成したリポジトリにモデルがアップロードできたところまで確認できたら、ダウンロードしたLLMは削除する。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>hf cache delete
</span></span></code></pre></div><h2 id=ollamaでllmをpullローカル実行>OllamaでLLMをpull&ローカル実行</h2><h3 id=ollamaのインストール>Ollamaのインストール</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install --cask ollama
</span></span></code></pre></div><h3 id=ollamaを起動>Ollamaを起動</h3><p>一度アイコンから起動しないと<code>ollama</code>コマンドが実行できなかった。<br><img src=../assets/application_Icon_of_ollama.png alt=|200></p><h3 id=llmをpull>LLMをpull</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># チャット、ドキュメント作成支援用</span>
</span></span><span style=display:flex><span>ollama pull hf.co/shindy-dev/Llama-3-shindy-jp-8B-GGUF:Q4_K_M
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ドキュメントの類似検索用</span>
</span></span><span style=display:flex><span>ollama pull hf.co/shindy-dev/granite-embedding-278m-shindy-multilingual-GGUF:Q8_0
</span></span></code></pre></div><p>他にpullしたいモデルがあれば以下を雛形としてpullすること。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ollama pull &lt;モデルのリポジトリURL&gt;
</span></span></code></pre></div><h2 id=obsidianの拡張機能でllmを実行>Obsidianの拡張機能でLLMを実行</h2><h3 id=smart-composer>Smart Composer</h3><p>ObsidianでAIとチャットできる拡張機能。チャットを通じて編集中のドキュメントについて執筆をサポートしてくれる。<br>公式ドキュメント：<a href=https://github.com/glowingjade/obsidian-smart-composer/wiki>Home · glowingjade/obsidian-smart-composer Wiki</a></p><p>設定画面で<code>ollama pull</code>したモデルを設定する。
<img src=../assets/Pasted%20image%2020250817000235.png alt=600>
<img src=../assets/Pasted%20image%2020250817000313.png alt=600></p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>作成したAIモデル、データセット、アプリを管理&公開できるGitHubライクなプラットフォーム&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>ローカルでAIモデルを動かすためのプラットフォーム&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div><ul class=pa0><li class="list di"><a href=/shindys-note/tags/huggingface/ class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">HuggingFace</a></li><li class="list di"><a href=/shindys-note/tags/ollama/ class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Ollama</a></li><li class="list di"><a href=/shindys-note/tags/obsidian/ class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Obsidian</a></li></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"><div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links"><p class="f5 b mb3">Related</p><ul class="pa0 list"><li class=mb2><a href=/shindys-note/obsidian%E7%92%B0%E5%A2%83%E6%A7%8B%E7%AF%89/></a></li></ul></div></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href=https://shindy-dev.github.io/shindys-note/>&copy; Shindy's Note 2025</a><div><div class=ananke-socials></div></div></div></footer></body></html>